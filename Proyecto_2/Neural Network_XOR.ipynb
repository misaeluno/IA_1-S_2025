{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RED NEURAL CON BACKPROPAGATION\n",
    "\n",
    "<img src=\"NN_XOR_Image1.png\" style=\"width:50%;height:85%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Activación\n",
    "<img src=\"Sigmoidal.png\" style=\"width:30%;height:40%;\">\n",
    "\n",
    "#### Sigmoidal\n",
    "$g(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "#### Derivada de la función sigmoidal\n",
    "$g'(z)=g(z)(1-g(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "\n",
    "# Función de activación\n",
    "#=======================\n",
    "# Sigmoidal\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoidal\n",
    "def s_prime(z):\n",
    "    return np.multiply(z, 1.0-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de los pesos\n",
    "#=============================\n",
    "def init_weights(layers, epsilon):\n",
    "    weights = []\n",
    "    for i in range(len(layers)-1):\n",
    "        w = np.random.rand(layers[i+1], layers[i]+1)\n",
    "        w = w * 2*epsilon - epsilon\n",
    "        weights.append(np.mat(w))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagación Hacia Adelante\n",
    "\n",
    "Sea:\n",
    "\n",
    "$a_{i}^{(j)}$: activación de la unidad $i$ en la capa $j$\n",
    "\n",
    "$\\Theta^{(j)}=w^{(j)}$: Matriz de pesos que controlan el mapeo de funciones desde la capa $𝑗$ a la capa $𝑗+1$\n",
    "\n",
    "Conjunto de entrenamiento: {$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$}\n",
    "\n",
    "Para nuestra red neuronal:\n",
    "\n",
    "$x=\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Entonces:\n",
    "\n",
    "$a^{(1)}=x$\n",
    "\n",
    "$z^{(2)}=\\Theta^{(1)}a^{(1)}$\n",
    "\n",
    "$a^{(2)}=g(z^{(2)})$\n",
    "\n",
    "$z^{(3)}=\\Theta^{(2)}a^{(2)}$\n",
    "\n",
    "$a^{(3)}=h_\\Theta(x)=g(z^{(3)})$\n",
    "\n",
    "### Propagación Hacia Atrás\n",
    "\n",
    "$\\delta^{(3)}=a^{(3)}-y$\n",
    "\n",
    "$\\delta^{(2)}=(\\Theta^{(2)})^{\\top}\\delta^{(3)}.*g'(z^{(2)})$\n",
    "\n",
    "No hay error en $\\delta^{(1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Neuronal\n",
    "#==============\n",
    "def fit(X, Y, w):\n",
    "    # Inicialización de cada parámetro con gradiente igual a 0\n",
    "    w_grad = ([np.mat(np.zeros(np.shape(w[i])))\n",
    "              for i in range(len(w))])  # len(w) es igual al número de capas\n",
    "    m, n = X.shape\n",
    "    h_total = np.zeros((m, 1))  # Valor predecido de todas las muestras, m*1, probabilidad\n",
    "    for i in range(m):\n",
    "        x = X[i]\n",
    "        y = Y[0,i]\n",
    "        # Propagación hacia adelante\n",
    "        #============================\n",
    "        a = x\n",
    "        a_s = []\n",
    "        for j in range(len(w)):\n",
    "            a = np.mat(np.append(1, a)).T\n",
    "            a_s.append(a)  # Aquí se guarda el valor a de la capa L-1 anterior.\n",
    "            z = w[j] * a\n",
    "            a = sigmoid(z)\n",
    "        h_total[i, 0] = a\n",
    "        # Propagación hacia atras (backpropagation)\n",
    "        #===========================================\n",
    "        delta = a - y.T\n",
    "        w_grad[-1] += delta * a_s[-1].T  # Cradiente de la capa L-1\n",
    "        # Reverso, desde la penúltima capa hasta el final de la segunda capa, excluyendo la primera y la última capa\n",
    "        for j in reversed(range(1, len(w))):\n",
    "            delta = np.multiply(w[j].T*delta, s_prime(a_s[j]))  # El parámetro pasado aquí es a, No z\n",
    "            w_grad[j-1] += (delta[1:] * a_s[j-1].T)\n",
    "    w_grad = [w_grad[i]/m for i in range(len(w))]\n",
    "    J = (1.0 / m) * np.sum(-Y * np.log(h_total) - (np.array([[1]]) - Y) * np.log(1 - h_total))\n",
    "    return {'w_grad': w_grad, 'J': J, 'h': h_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de los parámetros de la red\n",
    "#========================================\n",
    "# Conjunto de entrenamiento\n",
    "X = np.mat([[0,0],\n",
    "            [0,1],\n",
    "            [1,0],\n",
    "            [1,1]])\n",
    "Y = np.mat([0,1,1,0])\n",
    "\n",
    "# Configuración de red\n",
    "layers = [2,2,1]\n",
    "epochs = 5000 #número de iteraciones\n",
    "alpha = 0.5 #tasa de aprendizaje\n",
    "epsilon = 1 #para inicializar los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de la red\n",
    "#========================\n",
    "w = init_weights(layers, epsilon)\n",
    "result = {'J': [], 'h': []}\n",
    "w_s = {}\n",
    "for i in range(epochs):\n",
    "    fit_result = fit(X, Y, w)\n",
    "    w_grad = fit_result.get('w_grad')\n",
    "    J = fit_result.get('J')\n",
    "    h_current = fit_result.get('h')\n",
    "    result['J'].append(J)\n",
    "    result['h'].append(h_current)\n",
    "    for j in range(len(w)):\n",
    "        w[j] -= alpha * w_grad[j]\n",
    "    if i == 0 or i == (epochs - 1):\n",
    "        # print('w_grad', w_grad)\n",
    "        w_s['w_' + str(i)] = w_grad[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de la Función de Costo, J\n",
    "#===================================\n",
    "plt.plot(result.get('J'))\n",
    "plt.xlabel('Número de iteraciones (epocas)')\n",
    "plt.ylabel('Función de Costo, J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados\n",
    "#============\n",
    "from IPython.display import Math\n",
    "\n",
    "#print(\"Matrices de pesos inicial (aleatorias) y final (despues de iteraciones):\")\n",
    "#print(w_s)\n",
    "\n",
    "for i in [0, epochs-1]:\n",
    "    if i==0:\n",
    "        print('PESOS INICIALES ALEATORIOS')\n",
    "        print('==========================')\n",
    "    if i==(epochs-1):\n",
    "        print('PESOS FINALES DE RED NEURONAL ENTRENADA')\n",
    "        print('=======================================')        \n",
    "    j = len(w_s)\n",
    "    for j in range(1,j+1):\n",
    "        display(Math(r'\\Theta^{(%d)} = ' % (j)))\n",
    "        print(w_s['w_' + str(i)][j-1]) #w_s['w_0'][0],w_s['w_0'][1], w_s['w_4999'][0], w_s['w_4999'][1]\n",
    "    print('')\n",
    "\n",
    "print('PREDICCIÓN INICIAL')\n",
    "print('==================')\n",
    "display(Math(r'h_\\Theta(x) = ' ))\n",
    "print(result.get('h')[0])\n",
    "\n",
    "print('')\n",
    "\n",
    "print('PREDICCIÓN FINAL ESTIMADA')\n",
    "print('=========================')\n",
    "display(Math(r'h_\\Theta(x) = ' ))\n",
    "print(result.get('h')[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
